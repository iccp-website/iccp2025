Timestamp,Speaker last name,Speaker first name,Affiliation,Talk title,Abstract,Bio,"I am NOT available to speak at the following times:
(please use this only if you have exceptional constraints, we may not be able to accommodate all requests)"
5/9/2025 13:12:45,Waller,Laura,UC Berkeley,Space-time methods for super-resolution microscopy of dynamic samples,"Computational imaging is permeating cameras and microscopes across many scientific applications, enabling new high-resolution and multi-dimensional measurement capabilities (e.g. phase, 3D, hyperspectral). But many methods require acquisition of multiple images to reconstruct this new information, limiting their applicability for live dynamic samples, where motion blur can cause severe artifacts. This talk will describe new space-time algorithms that correct for motion artifacts and solve for dynamics, with imperfect optical systems or approximate forward models. Traditional model-based image reconstruction algorithms work together with neural networks to optimize the inverse problem solver and the data capture strategy.","Laura Waller is the Charles A. Desoer Professor of Electrical Engineering and Computer Sciences at UC Berkeley. She received B.S., M.Eng. and Ph.D. degrees from the Massachusetts Institute of Technology in 2004, 2005 and 2010. After that, she was a Postdoctoral Researcher and Lecturer of Physics at Princeton University from 2010-2012. She is a Packard Fellow for Science & Engineering, Moore Foundation Data-driven Investigator, OSA Fellow, and Chan-Zuckerberg Biohub Investigator. She has received the Carol D. Soc Distinguished Graduate Mentoring Award, OSA Adolph Lomb Medal, the SPIE Early Career Award and the Max Planck-Humboldt Medal. ",
5/11/2025 20:43:11,Freeman,Bill,MIT,The Sensor Independence Assumption,"For imaging problems in science or medicine, you'd like to rely on prior assumptions about images as little as possible, when making image reconstructions from underdetermined sensor measurements. One often imposes the constraint that light is positive, which doesn't impose any disputable prior assumption about the images we're reconstructing. There's another indisputable assumption that I think is under-exploited:  statistical independence between the sensor and the world.  I'll show the power of this assumption for some underdetermined linear inverse problems.","William T. Freeman is the Thomas and Gerd Perkins Professor of Electrical Engineering and Computer Science (EECS) at MIT. He is interested in mid-level vision, audio, and computational photography, and co-authored a computer vision textbook recently published by MIT Press.   He received outstanding paper awards at computer vision or machine learning conferences in 1997, 2006, 2009,  2012 and 2019, and test-of-time awards for papers from 1990, 1995, 2002, 2005, and 2012.  He is a member of the National Academy of Engineering, and a Fellow of the IEEE, ACM, and AAAI.  In 2019, he received the PAMI Distinguished Researcher Award, the highest award in computer vision.","Wednesday, July 23, PM"
5/12/2025 13:55:09,Horstmeyer,Roarke,Duke University,High-throughput computational imaging with multi-camera array microscopes,"This talk details several new imaging approaches for extremely high-throughput capture of images and video at the microscopic scale. Due to their limited field-of-view, today’s microscopes are relatively inefficient at imaging dynamics (e.g., living organisms, live cells) across large areas and volumes. This is particularly true for 3D recordings of freely moving small model organisms like the zebrafish and fruit fly, as well as in vitro cultures such as organoids and organospheres. This work presents a multi-camera array microscope (MCAM) that contains dozens of tightly packed imaging systems that can synchronously record video at multiple gigabytes per second data rates. In this talk, I will highlight several unique imaging configurations that are optimized to operate with the MCAM platform to unlock new scientific measurement regimes and large speed and efficiency gains for high-throughput imaging workflows. ","Roarke Horstmeyer is an assistant professor of Biomedical Engineering and Electrical and Computer Engineering at Duke University. He is also the Scientific Director at Ramona Optics. He develops microscopes, cameras and computer algorithms for a wide range of applications, from forming large-area, high-resolution 3D videos of freely moving organisms to detecting blood flow and brain activity deep within tissue. Dr. Horstmeyer’s lab currently performs research within the fields of ptychography, high-content microscopic imaging, physics informed machine learning algorithms, and biophotonic measurement systems. Before joining Duke in 2018, Dr. Horstmeyer was a visiting professor at the University of Erlangen in Germany and an Einstein International Postdoctoral Fellow at Charité Medical School in Berlin. Prior to his time in Germany, Dr. Horstmeyer earned a PhD from Caltech’s Electrical Engineering department (2016), an MS from the MIT Media Lab (2011), and bachelor’s degrees in Physics and Japanese from Duke in 2006.","Wednesday, July 23, AM, Wednesday, July 23, PM"
5/13/2025 4:23:53,Veeraraghavan,Ashok,Rice University,"Optics, sensors and AI: synergic computational imaging to go beyond the limits imposed by conventional imaging","In this talk, I will discuss about several projects in my lab at the confluence of optics, sensors and artificial intelligence. In particular, I will provide examples of how co-designing sensors, optics and AI algorithms results in superior performance capabilities for imaging systems. I will provide a few example projects: (1) how co-designing imaging optics along with AI algorithms can enable high-throughput 3D imaging, and microscopy, (2) how novel diffractive and meta-optical elements allow us to realize imaging systems with novel functionalities and form-factors and finally time-permitting, (3) how emerging neural representations along with high resolution spatial light modulators can allow us to image through thick scattering media without the need for guidestars. I will use these projects to argue that we should look at the three computational blocks within an imaging system, optics, sensors and algorithms together and that co-designing them can result in significant performance improvements over the state of art.","Ashok Veeraraghavan serves as the current Chair of the Department of Electrical and Computer Engineering. He also directs the computational imaging lab, which focuses on solving hard and challenging problems in imaging and vision by co-designing sensors, optics, electronics, signal processing, and machine learning algorithms. Before joining Rice University, he spent three wonderful and fun-filled years as a Research Scientist at Mitsubishi Electric Research Labs in Cambridge, MA. He received his Bachelors in Electrical Engineering from the Indian Institute of Technology, Madras in 2002 and M.S and PhD. degrees from the Department of Electrical and Computer Engineering at the University of Maryland, College Park in 2004 and 2008 respectively. His thesis received the Doctoral Dissertation award from the Department of Electrical and Computer Engineering at the University of Maryland. His work has won numerous awards including the Peter and Edith O'Donnel Award for Engineering in 2024, Charles Duncan Innovation Award 2019, Hershel. M. Rich Invention Award in 2016 and 2017, and an NSF CAREER award in 2017. He loves playing, talking, and pretty much anything to do with the slow and boring but enthralling game of cricket.","Monday, July 21, AM, Monday, July 21, PM, Wednesday, July 23, PM"
5/13/2025 10:32:12,Chowdhury,Shwetadwip,UT Austin,Exploring the nonconvex landscape of biological inverse-scattering,"Optical imaging is a major research tool in the basic sciences, but is limited by tissue scattering to short imaging depths. This prevents large-scale bio-imaging by allowing visualization of only the outer superficial layers of an organism, or specific components isolated from within the organism and prepared in-vitro.  Our lab develops inverse-scattering techniques to computationally correct for the effects of tissue scattering and reconstruct high-resolution volumetric images of the sample. However, these methods are nonconvex and susceptible to local minima. We demonstrate that reconstruction quality is critically, and sometimes counterintuitively, influenced by factors such as the choice between field- and amplitude-based cost functions, computational padding, initialization strategies using weak-scattering assumptions, and the application of refocusing. Lastly, we present the methodology that we found through empirical testing has proven most effective for robust and high-quality 3D imaging of diverse biological scattering samples.",Shwetadwip Chowdhury is an assistant professor in the Department of Electrical and Computer Engineering at the University of Texas at Austin. His research interests are in developing next generation optical imaging technologies for applications in science and medicine. ,
5/14/2025 3:07:29,Piestun,Rafael,University of Colorado Boulder,Computational Imaging enables ultra-thin endomicroscopy,"In-vivo imaging using hair-thin multimode fibers has been recently accomplished. Multimode fibers are attractive for endoscopic applications due to their thin cross-section, high modal density, and flexibility. However modal dispersion and intermodal coupling preclude direct image transmission. Computational imaging techniques are thus critical to enable full functionality. The development of fast spatial phase control enables focus scanning and structured illumination for different imaging modalities. We discuss the implications of these techniques for ultrathin optical endoscopy and show the latest in-vivo implementations.
","Prof. Rafael Piestun received the Ingeniero Electricista degree from the Universidad de la República (Uruguay) and MSc. and Ph.D. degrees in Electrical Engineering from the Technion – Israel. From 1998 to 2000 he was a researcher at Stanford University. Since 2001 he has been at the University of Colorado Boulder where he is a professor in the department of Electrical and Computer Engineering and in the Physics department. He is a fellow of the Optical Society of America, was a Fulbright scholar, an Eshkol fellow, received a Honda Initiation Grant award, a Minerva award, a Provost Achievement Award, and El-Op and Gutwirth prizes. He was associate editor of Optics and Photonics News and Applied Optics. He is founder of the company Double Helix Optics (SPIE Prism Award, First Place in the Luminate Competition) and the company Modendo Inc. His areas of interest include computational optical imaging, superresolution microscopy, volumetric photonic devices, scattering optics, and ultrafast optics.","Wednesday, July 23, AM, Wednesday, July 23, PM"
5/14/2025 18:28:46,Goyal,Vivek,Boston University,Quantitative Secondary Electron Yield Mapping in Ion-Beam Microscopy,"Secondary electron imaging (SEI) modalities, such as scanning electron microscopy (SEM) and helium ion microscopy (HIM), are widely used for nanoscale visualization. They are known to suffer from super-Poissonian noise and are generally qualitative because of unknown detector parameters. My group has been working to develop SEI as a quantitative modality with robustness to various sources of noise. The central idea is to recognize that there are phenomena within each pixel dwell time that make it worthwhile to measure more than a single scalar value during that dwell time. Recently, we have shown how to use our central idea with an existing HIM instrument, without replacing its noisy Everhart-Thornley detector. With ion count-aided microscopy [10.1073/pnas.2401246121], we demonstrate an improvement in the dose-accuracy tradeoff that enables dose reduction by a factor between 2 and 3. Theory predicts much larger improvements are possible with heavier incident ions or in low-voltage SEM. Our improvement represents a near elimination of source shot noise, and ongoing work addresses beam spot size-related limitations to resolution.","Vivek Goyal received undergraduate degrees in electrical engineering and mathematics from the University of Iowa.  After graduate degrees in electrical engineering from UC-Berkeley, he was a Member of Technical Staff at Bell Laboratories, a Senior Research Engineer for Digital Fountain, and the Esther and Harold E. Edgerton Associate Professor of Electrical Engineering at MIT.  His research group spawned 3dim Tech, winner of the 2013 MIT $100K Entrepreneurship Competition Launch Contest Grand Prize, and he was consequently with Google/Alphabet Nest Labs 2014-2016.  He is now a Professor of Electrical and Computer Engineering at Boston University.  Dr. Goyal is a Fellow of the AAAS, IEEE, and Optica, and he and his students have been awarded ten IEEE paper awards and eight thesis awards.  He is a co-author of Foundations of Signal Processing (Cambridge Univ. Press, 2014) and a 2024 Guggenheim Fellow.",
5/14/2025 23:36:23,O'Toole,Matthew,Carnegie Mellon University,Spatially-selective Lensing,"A camera lens brings a single plane into focus on a sensor; parts of the scene not on this plane are resolved on the sensor under defocus. Can we break this precept by enabling a lens that changes its depth-of-field arbitrarily? This presentation discusses the design and implementation of a lens capable of spatially-selective focusing.  Through a combination of cubic phase plates and spatial light modulators, we demonstrate control over focus at the pixel level. This confers unique imaging capabilities---most notably, the ability to bring an entire scene into sharp focus without reducing the size of the lens's aperture.","Matthew O'Toole is an Associate Professor with the Robotics Institute and Computer Science Department at Carnegie Mellon University. Prior to joining CMU, he was a Banting Postdoctoral Scholar at Stanford University, and received his Ph.D. from the University of Toronto. He is also the recipient of an NSF CAREER award; a ACM SIGGRAPH Outstanding Thesis Honourable Mention award; best paper awards at SIGGRAPH 2023, CVPR 2022, CVPR 2014, and ICCV 2007; and best demo awards at ICCP 2023, CVPR 2015, and ICCP 2015. ",
5/16/2025 14:32:18,Gao,Liang,UCLA,Breaking the Speed Barrier: High-Speed Light-Field Microscopy for Kilohertz 3D Imaging and beyond,"Light-field microscopy offers unprecedented capabilities for 3D visualization of biological samples down to the sub-cellular level. Despite its strengths, the technique has historically faced limitations in imaging speed due to the acquisition of large-format light field data. This bottleneck has restricted its use in high-speed bioimaging applications like voltage imaging or fluorescence lifetime imaging microscopy (FLIM). In this presentation, I will discuss our innovative approaches to overcoming this challenge by leveraging advancements in computational optics and detector technology, enabling a volumetric frame rate from kilohertz to terahertz.","Dr. Liang Gao is an Associate Professor of Bioengineering at UCLA, specializing in optical imaging and its application in biomedicine. He has authored over 80 peer-reviewed publications in top-tier journals, including Nature, Nature Communications, Science Advances, and PNAS. Dr. Gao received his BS degree in Physics from Tsinghua University in 2005 and his PhD in Applied Physics and Bioengineering from Rice University in 2011. He has been honored with several awards, including a CAREER award from NSF, a MIRA award from NIH, a Frontiers of Science Award in Computational Optics from ICBS, and a Harold E. Edgerton Award in High-Speed Optics from SPIE. Dr. Gao is also a fellow member of SPIE and Optica (formerly OSA).","Tuesday, July 22, AM, Tuesday, July 22, PM, Wednesday, July 23, AM, Wednesday, July 23, PM"
5/16/2025 16:41:31,Adelson,Edward,Massachusetts Institute of Technology,Capturing touch and surface geometry with vision based tactile sensors,"A vision based tactile sensor (VBTS) consists of a soft gel pad covered with a reflective membrane, viewed by an internal camera. My lab builds GelSight sensors, which capture the distortion of the membrane though photometric stereo using internal LED’s. The native language of a VBTS is geometry: when the sensor presses on a surface we can track the X,Y,Z distortion of every point on the membrane over time. With a GelSight robot fingertip we can measure force, but usually geometry is more important than force. The soft robot fingertip lets us detect slip, identify objects and materials, determine object pose, and estimate material properties like hardness. GelSight-based instruments can also be used to measure surface microgeometry with optically challenging materials (like glass or metal) and challenging circumstances (like on the wing of an airplane).","Edward Adelson is the John and Dorothy Wilson Professor of Vision Science at MIT, in the Department of Brain and Cognitive Sciences, and a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL). He is a member of the National Academy of Sciences. He has published widely on topics in human vision, computer vision, computer graphics, artificial tactile sensing, and robotics. He is well known for contributions to multiscale image representation (such as the Laplacian pyramid) and basic concepts in early vision such as steerable filters and motion energy models. He introduced the plenoptic function, and built the first plenoptic camera. He has produced some well known illusions such as the Checker-Shadow Illusion. He has been honored with the Nakayama Medal of the Vision Sciences Society,  two test-of-time awards from the IEEE Computer Society, and the IEEE Lifetime Achievement Award in Computer Vision. He now works on artificial touch sensing for robotics.",
5/17/2025 15:42:21,Nayar,Shree,Columbia University,Vision for Solar Energy,"In urban areas, solar panels are mounted on rooftops, on the sides of buildings, at the ground level, on poles, and even embedded within windows. The market for urban solar panels was 64 billion is 2025 and is projected to double by 2035. What makes the use of a solar panel in an urban setting interesting is that, due to occlusions, the panel sees only a fraction of the sky and hence receives direct sunlight for only a part of the day, and maybe even never. An important question therefore is: For any given urban location, how should a panel be oriented to maximize its irradiance and hence the energy it harvests? We present a minimal sensing method for automatically orienting a solar panel in the direction of maximum irradiance, irrespective of the complexity of the illumination it receives. We also present a vision-based approach for forecasting the energy harvested by an already installed panel, which is valuable information for managing supply and demand in an electric grid. ","Shree K. Nayar is the T. C. Chang Professor of Computer Science at Columbia University. He heads the Columbia Imaging and Vision Laboratory (CAVE), which develops computational imaging and computer vision systems. Nayar received his PhD degree in Electrical and Computer Engineering from the Robotics Institute at Carnegie Mellon University. For his research and teaching he has received several honors including the David Marr Prize (1990 and 1995), the David and Lucile Packard Fellowship (1992), the National Young Investigator Award (1993), the NTT Distinguished Scientific Achievement Award (1994), the Keck Foundation Award for Excellence in Teaching (1995), the Columbia Great Teacher Award (2006), the Carnegie Mellon Alumni Achievement Award (2009), Sony Appreciation Honor (2014), the Columbia Engineering Distinguished Faculty Teaching Award (2015), the IEEE PAMI Distinguished Researcher Award (2019), the Funai Achievement Award (2021), and the Okawa Prize (2022). For his contributions to computer vision and computational imaging, he was elected to the National Academy of Engineering in 2008, the American Academy of Arts and Sciences in 2011, and the National Academy of Inventors in 2014.
","Tuesday, July 22, PM, Wednesday, July 23, AM, Wednesday, July 23, PM"
5/18/2025 18:00:37,Gupta,Mohit,University of Wisconsin-Madison,Quanta Computational Imaging,"Single-photon avalanche diodes (SPADs) are an emerging sensor technology that promises single-photon sensitivity and the ability to time-tag photons with picosecond precision. In the last few years, we have witnessed a single-photon revolution, resulting for the first time in multi-megapixel SPAD arrays, which allow capturing sufficient spatial details needed for mainstream computer vision applications, including object detection and tracking, text recognition, pose recognition, and SLAM. This raises the following questions: Are SPADs ready to be used beyond niche scientific applications, as general-purpose computer vision sensors? Can they be deployed outside the lab in-the-wild? If so, what are their benefits over conventional cameras? We will discuss the opportunities for (and open challenges in) developing ``quanta’’ imaging / perception algorithms for extracting scene information from SPAD photon streams, with the goal of performing robust vision in extreme conditions including rapid motion, low light (~0.1 lux) and high dynamic range. ","Mohit Gupta is an Associate Professor of Computer Sciences at the University of Wisconsin-Madison. He received a Ph.D. from the Robotics Institute, Carnegie Mellon University, and was a postdoctoral research scientist at Columbia University. He directs the WISION Lab with research interests in computer vision and computational imaging. He has received best paper honorable mention awards at computer vision and photography conferences, including a Marr Prize honorable mention at IEEE ICCV, a best demo award at SIGGRAPH E.Tech. 2024, Sony Faculty Innovation Awards and an NSF CAREER award. His research is supported by NSF, ONR, DARPA, Sony, Snap, and Intel.","Wednesday, July 23, AM, Wednesday, July 23, PM"
5/22/2025 10:04:30,Narasimhan,Srinivasa,Carnegie Mellon University,Driving in Bad Weather and through Workzones,"Driver-assist and autonomous driving systems have come a long way since the DARPA grand challenges but we still keep hearing of accidents in difficult conditions including bad weather, night and work zones. Thousands of fatalities happen in such conditions every year even in regions and countries where transportation infrastructure is advanced. In this talk, I will present some of our group's works on how to generate and understand road scenes in such conditions by leveraging and controlling large generative models and augmenting with diverse data we have been collecting. ","Srinivasa Narasimhan is the U.A. and Helen Whitaker Professor of Robotics at Carnegie Mellon University. He currently serves as Interim Director of the Robotics Institute leading a department of over a thousand students, staff and faculty. He obtained his PhD from Columbia University in Dec 2003. His group focuses on novel techniques for imaging and illumination to enable applications in vision, graphics, robotics, agriculture, intelligent transportation and medical imaging. His works have received over a dozen Best Paper or Best Demo or Honorable mention awards at major conferences [IV (2021), ICCV (2013), CVPR (2022, 2019, 2015, 2000), ICCP (2020, 2015, 2012), I3D (2013), CVPR/ICCV Workshops (2007, 2009)]. In addition, he has received the Ford URP Award (2013), Okawa Research Grant (2009) and the NSF CAREER Award (2007). He is the co-inventor of programmable headlights, Aqualux 3D display, Assorted-pixels, Motion-aware cameras, Episcan360, Episcan3D, EpiToF3D, and programmable triangulation light curtains. 
Hi served on the editorial board of the International Journal of Computer Vision (2009-2023) and serves frequently as Senior or Lead Area Chair of top computer vision conferences (CVPR, ICCV, ECCV, BMVC, ACCV, 3DV).","Monday, July 21, AM, Wednesday, July 23, PM"
5/22/2025 15:52:39,Wetzstein,Gordon,Stanford University,Holographic Near-eye Displays,"Holographic near-eye displays are an emerging platform for virtual and augmented reality systems. Their unique ability to control the phase of a displayed wavefront enables aberrations of downstream optics, such as waveguide combiners, to be corrected by the light engine. This ability opens up a new design space for ultra-compact AR/VR display systems. In this talk, we review recent advances in compact holographic display systems with large etendue, efficient AI-based calibration techniques of these systems, and modern computer-generated holography algorithms that leverage Gaussian splatting for wave optics-based holographic displays.","Gordon Wetzstein is an Associate Professor of Electrical Engineering and, by courtesy, of Computer Science at Stanford University. He is the director of the Stanford Computational Imaging Lab and a faculty director of the Stanford Center for Image Systems Engineering. At the intersection of computer graphics and vision, artificial intelligence, computational optics, and applied vision science, Prof. Wetzstein's research has a wide range of applications in next-generation imaging, wearable computing, and neural rendering systems. Prof. Wetzstein is a Fellow of Optica and the IEEE, and the recipient of several awards, including an IEEE VGTC Virtual Reality Technical Achievement Award, an NSF CAREER Award, an Alfred P. Sloan Fellowship, an ACM SIGGRAPH Significant New Researcher Award, a Presidential Early Career Award for Scientists and Engineers (PECASE), an SPIE Early Career Achievement Award, an Electronic Imaging Scientist of the Year Award, an Alain Fournier Ph.D. Dissertation Award as well as several Best Paper and Demo Awards.","Wednesday, July 23, AM, Wednesday, July 23, PM"
5/25/2025 10:52:40,Heidrich,Wolfgang,KAUST,Learned and Domain Specific Imaging Systems,"Computational imaging systems are based on the joint design of optics and associated image reconstruction algorithms. Of particular interest in recent years has been the development of end-to-end learned “Deep Optics” systems that use differentiable optical simulation in combination with backpropagation to simultaneously learn optical design and deep network post-processing for applications such as hyperspectral imaging, HDR, or extended depth of field. In this talk I will in particular focus on new developments that expand the design space of such systems from simple DOE optics to compound refractive optics and mixtures of different types of optical components. I will also highlight how to design application-specific imaging systems that require very low hardware resources using the power of generative models.","Wolfgang Heidrich is a Professor of Computer Science and Electrical and Computer Engineering in the KAUST Visual Computing Center, for which he also served as director from 2014 to 2021. Prof. Heidrich joined King Abdullah University of Science and Technology (KAUST) in 2014, after 13 years as a faculty member at the University of British Columbia. He received his PhD in from the University of Erlangen in 1999, and then worked as a Research Associate in the Computer Graphics Group of the Max-Planck-Institute for Computer Science in Saarbrucken, Germany, before joining UBC in 2000. Prof. Heidrich’s research interests lie at the intersection of imaging, optics, computer vision, computer graphics, and inverse problems. His more recent interest is in computational imaging, focusing on hardware-software co-design of the next generation of imaging systems, with applications such as High-Dynamic Range imaging, compact computational cameras, hyperspectral cameras, to name just a few. Prof. Heidrich is a Fellow of the National Academy of Inventors, IEEE, Optica, AAIA, and Eurographics, and the recipient of a Humboldt Research Award as well as the ACM SIGGRAPH Computer Graphics Achievement Award.",
5/25/2025 23:48:04,Gkioulekas,Ioannis,Carnegie Mellon University,A ray tracer for physics,"Ray tracing has revolutionized science and engineering, by enabling the simulation of light at unprecedented scale and generality. Yet how we simulate light is in stark contrast with how we simulate other physical phenomena, modeled through general partial differential equations, for which methods such as finite elements have been the gold standard for decades. The reliance of these methods on grid-based discretization fundamentally limits how much we can scale or generalize physics simulation, in turn hindering progress across all areas of science and engineering. I will discuss ongoing work to change this state of affairs, by creating a new methods for simulation of partial differential equations that achieve the same scale and generality as ray tracing. As with ray tracing, the key insight behind these algorithms is to elide discretization through Monte Carlo estimation. Our Monte Carlo simulation methods are embarrassingly parallelizable, scale to massive scene specifications, enable differentiability and volumetrization, and create exciting new opportunities for utilizing simulation in computational imaging.","Ioannis Gkioulekas is an associate professor in the Robotics Institute at Carnegie Mellon University. He works broadly in computer graphics and computer vision, focusing on computational imaging: the joint design of optics, electronics, and computation, to achieve unprecedented imaging capabilities. Technical keywords that often show up in his research include: interferometry, acousto-optics, single-photon imaging, lidar, speckle, physics-based rendering, differentiable rendering, volume rendering, Monte Carlo simulation. For his research he has received the NSF CAREER Award, Sloan Research Fellowship, Bodossaki Distinguished Young Scientist Award, and three best paper awards at CVPR and SIGGRAPH.",
5/26/2025 9:37:21,Sankaranarayanan,Aswin,Carnegie Mellon University,PlatonicVR: Exploiting Human Visual Properties for Ultra-Wide Field of View Displays,"Matching the formidable capabilities of the human visual system remains one of the enduring challenges in display research. A fundamental limitation in current stereoscopic displays is the coupling between eyebox extent and field of view, which arises from the eye's use of rotational motion to explore its visual field. This coupling prevents conventional approaches from achieving the ~200° horizontal field of view of human vision without prohibitive complexity.
In this talk, I will present PlatonicVR, a novel stereoscopic display architecture that exploits specific properties of human visual perception to dramatically simplify ultra-wide field of view display design. By leveraging these perceptual characteristics, our approach decouples the eyebox-field of view relationship and enables practical implementation of displays that approach the human eye's natural viewing capabilities. I will discuss the underlying optical principles, demonstrate the effectiveness of our approach, and explore implications for next-generation VR systems.","Aswin Sankaranarayanan is a professor in the ECE department at CMU, where he leads the Image Science Lab. His research interests are broadly in computational photography, signal processing and computer vision. His doctoral research was in the University of Maryland where his dissertation won the distinguished dissertation award from the ECE department in 2009. Aswin is also the recipient of the NSF CAREER award, and  best paper awards at SIGGRAPH, CVPR and ICCP.
","Wednesday, July 23, AM, Wednesday, July 23, PM"
5/26/2025 19:56:53,Heide,Felix,Princeton University,Cameras As Nanophotonic Optical Computers,"Presentation Abstract: Cameras have become a ubiquitous interface between the real world and computers, with applications across domains in fundamental science, robotics, health, and communication. Although their applications are diverse, today’s cameras acquire information in the same way they did in the 19th century: they focus light from the scene on a sensing plane using a set of refractive lenses that minimize deviations from Gauss’s linear model of optics. In this paradigm, increasingly complex lens and sensor stacks are designed to record an ideal image and perform computation only after the capture. For example, the optical stack in the iPhone 16 contains more than seven elements at 7 mm in length. In this talk, I will discuss computational cameras that learn to manipulate the wavefront of incident light with wavelength-scale structuring, previously impractical to design with existing electromagnetic wave simulation methods. These neural nanophotonic cameras may enable unprecedented capabilities in optical design, imaging, and computer vision. As examples, I will describe an ultra-small camera at a few hundred microns in size that matches the quality achieved with cm-size compound lenses. I will also present ultra-thin nanophotonic cameras that perform 99.9% of neural network compute - typically executed in electronics after the capture - in the optics before sensing, at the speed of light.","Felix Heide is an Assistant Professor at Princeton University, Head of AI at Torc Robotics, and Founder self-driving vehicle startup Algolux (now part of Diamler Trucks and Torc Robotics). He is researching the theory and application of computational imaging and computer vision systems. Exploring imaging, vision, and display systems end-to-end, Felix's work lies at the intersection of optics, machine learning, optimization, computer graphics, and computer vision. He received his Ph.D. from the University of British Columbia. Felix obtained his undergraduate degree from the University of Siegen and was a postdoc at Stanford University. His doctoral dissertation won the Alain Fournier Dissertation Award and the SIGGRAPH outstanding doctoral dissertation award. He won the NSF CAREER Award 2021 and the Sony Young Faculty Award 2021. He was named a Packard Fellow in 2022 and a Sloan Research Fellow in 2023. Felix was named SIGGRAPH New Significant Researcher in 2023. He founded the autonomous driving startup Algolux, building on academic research during his Ph.D. thesis. With over 150 employees and venture capital of more than 30 Million USD, Algolux has shipped computer vision software to more than 1 Million vehicles of European automotive manufacturers. ","Monday, July 21, AM, Wednesday, July 23, AM, Wednesday, July 23, PM"
6/4/2025 11:12:03,Süsstrunk,Sabine,EPFL,Neural Cellular Automata: A New Paradigm for Self-Organizing Textures,"In nature, complex patterns such as animal coats, wood grain, and swirling clouds emerge when many simple elements locally interact. Cellular automata (CAs) capture this principle, and neural cellular automata (NCAs) advance it further by learning the local update rule directly from data while preserving locality and parallelism. With carefully designed architectures, we build NCAs that generate rich, evolving 2D and 3D textures in real time. The models are lightweight and run efficiently even on edge devices, thus enabling real-time interactive user control. Our approach not only simplifies the creation and control of intricate visual effects but also prove how NCAs offer a powerful, fundamentally new tool set for computer graphics and texture synthesis.","Sabine Süsstrunk is Full Professor and Director of the Image and Visual Representation Lab in the School of Computer and Communication Sciences (IC) at the Ecole Polytechnique Fédérale (EPFL), Lausanne, Switzerland. Her main research areas are in computational photography and imaging, color computer vision, and computational image quality and aesthetics. Sabine is a Fellow of ELLIS, IEEE and IS&T, and President of the Swiss Science Council (SSC). She organized ICCP 2024 in Lausanne, Switzerland.","Wednesday, July 23, PM"
6/4/2025 23:45:48,Preza,Chrysanthe,The University of Memphis,Computational imaging enables efficient structured illumination microscopy,"Improving the performance of three-dimensional (3D) fluorescence microscopes is a topic that has received a lot of attention over the years. In this talk, I will discuss techniques that we have been developing to improve 3D-spatial resolution based on “optical-transfer function engineering” using novel structured illumination approaches and computational methods, including efficient image restoration using plug and play methods to integrate physical and learned models.","Dr. Chrysanthe Preza is the Kanuri Professor and Chair of the Department of Electrical and Computer Engineering at the University of Memphis, where she joined in 2006. She received her D.Sc. degree in Electrical Engineering from Washington University in St. Louis in 1998.  She leads the research in the Computational Imaging Research Laboratory. Since 2022, she is the founder and Director of the Vertically Integrated Projects (VIP) Program at The University of Memphis. Her research interests are imaging science, estimation theory, computational optical sensing and imaging applied to multidimensional multimodal light microscopy and hyperspectral imaging, and computational imaging enabled by deep learning. She received a CAREER award by the National Science Foundation in 2009, the Herff Outstanding Faculty Research Award in 2010 and 2015, and she was the recipient of the Ralph Faudree Professorship at the University of Memphis 2015-2018. Since 2018, she has been the recipient of the Ravi and Eleanor Kanuri Professorship. She was named Fellow of the SPIE in 2019 and Fellow of the Optica (OSA) in 2020. She served as Associate Editor for IEEE Transactions on Computational Imaging, as Topical Editor for Optica’s Applied Optics, and as an Executive Editor for Biological Imaging, Cambridge University Press. She currently serves as an Editorial Board Member for Journal of Imaging, Computational Imaging and Computational Photography section.","Wednesday, July 23, PM"
6/6/2025 16:05:17,Ye,Jinwei,George Mason University,Revisit Mirrors for 3D Imaging,"Mirrors are ubiquitous in imaging systems. In this talk, I’ll revisit various mirror configurations for 3D imaging. I’ll showcase a catadioptric lens design for snapshot 3D Gaussian Splatting and demonstrate 3D viewing results on hard-to-reconstruct tiny objects in motion, such as a crawling lady bug.","Jinwei Ye is an associate professor of Computer Science at George Mason University. Before that, she was an assistant professor at Louisiana State University (2017–2021). She received her Ph.D. in Computer Science from the University of Delaware in 2014. Her research interests are at the intersection of computer vision, computational imaging, and computer graphics. Her works are supported by NSF and ARL. She received the NSF CAREER awards in 2023. She served in the senior program committee and organizing committee for major computer vision conferences, including CVPR, ICCV, ECCV, WACV, and ICCP.",
6/7/2025 9:32:06,Patashnik,Or,Tel Aviv University,On Attention Layers for Image Generation and Manipulation,"Attention layers play a critical role in generative models. In this talk, I will show that these layers capture rich semantic information, and particularly semantic correspondences between elements within the image and across different images. Through several works, I will show that the rich representations learned by these layers can be leveraged for image manipulation, consistent image generation, and personalization. Additionally, I will discuss the challenges that arise, especially in scenarios involving complex prompts with multiple subjects. Specific issues, such as semantic leakage during the denoising process, can lead to inaccurate representations, resulting in poor generations. I will present methods that mitigate these issues, allowing the generation of complex images.","Or Patashnik is a Senior Lecturer (Assistant Professor) at the School of Computer Science at Tel Aviv University. Her research lies at the intersection of computer graphics, computer vision, and machine learning, with a particular focus on generative models. She develops methods for image and video generation, semantic editing, and personalization, aiming to make visual content creation more controllable and expressive.
Or completed her PhD at Tel Aviv University under the supervision of Prof. Daniel Cohen-Or. During her graduate studies, she also spent time at Carnegie Mellon University as a visiting scholar, collaborating with Prof. Fernando De la Torre and Prof. Jun-Yan Zhu. Her work has been recognized with several awards, including the Blavatnik Prize for Outstanding PhD Students, the Shashua Family Foundation Scholarship, and the Celia and Marcos Maus Annual Prize.","Monday, July 21, AM, Monday, July 21, PM, Wednesday, July 23, PM"
6/7/2025 19:11:22,Fridovich-Keil,Sara,Georgia Tech,Volume Representations for Inverse Problems,"We introduce a family of models, Geometric Algebra Planes, that is the first class of Implicit Neural Representation (INR) trainable by convex optimization. GA-Planes models generalize many existing representations, and have a natural interpretation as a low-rank plus low-resolution matrix factorization. In 3D, we demonstrate GA-Planes’ competitive performance in terms of expressiveness, model size, and optimizability across three volume fitting tasks including radiance field reconstruction. I will conclude with a benchmarking comparison of many INRs against a simple interpolated grid baseline, that highlights remaining limitations of INRs and open problems in 3D signal representation for inverse problems.","Sara Fridovich-Keil is an Assistant Professor at Georgia Tech in the School of Electrical and Computer Engineering, and program faculty in Machine Learning. Before joining Georgia Tech, she was a postdoc at Stanford and completed her PhD at UC Berkeley. Her research focuses on foundations and applications of machine learning and signal processing in computational imaging.","Monday, July 21, PM"