<section class="section container is-max-desktop content" id="Speakers">
	<h2 class="title">Invited Talks</h2>

	<section class="subsection">
		<h3 class="subtitle">Keynote Speakers</h3>
		<div class="box" id="edoardo" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://people.epfl.ch/edoardo.charbon">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/edoardo.png" alt="Image of Edoardo Charbon" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Edoardo Charbon</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">EPFL</p>
						<h5 class="title is-5" style="color: #3273dc;">Picosecond photography: from LiDAR to quantum imaging</h5>
					</div>
				</div>
			</div>
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>Solid-state photon-counting image sensors have emerged as useful tool to capture fast phenomena involving light at various wavelengths. Fast photon detection can be achieved with few picosecond resolutions using single-photon avalanche diodes (SPADs) [1, 2]. The impact of these detectors on light detection and ranging (LiDAR), fluorescence lifetime imaging microscopy (FLIM), Förster resonance energy transfer (FRET), time-of-flight positron emission tomography (TOF-PET), and many others has been remarkable, and more innovations in quantum imaging are expected. This will be enabled by advances in integrated SPADs along with powerful computational imaging techniques, such as quanta burst photography [3]. New technologies, such as 3D-stacking, Ge, and InP/InGaAs SPAD sensors, will accelerate adoption of SWIR/NIR image sensors [4]. A technological perspective will be given on computationally intensive image sensors, for affordable, yet powerful quantum imaging [5,6].<br><br>References<br>[1] K. Morimoto, M.-L. Wu, A. Ardelean, E. Charbon, "Superluminal Motion-Assisted Four-Dimensional Light-in-Flight Imaging", Phys. Rev. X 11, 011005 (2021). DOI: 10.1103/PhysRevX.11.011005<br>[2] F. Gramuglia, E. Ripiccini, C.A. Fenoglio, M.-L. Wu, L. Paolozzi, C. Bruschini, E. Charbon, "Direct MIP detection with sub-10 ps timing resolution Geiger-Mode APDs", Nucl. Instr. and Meth. In Phys. Res. A (NIMA) 1047, 167813 (2023). DOI: 10.1016/j.nima.2022.167813<br>[3] S. Ma, S. Gupta, A.C. Ulku, C. Bruschini, E. Charbon, and M. Gupta, "Quanta burst photography", ACM Trans. Graph. 39(4) 79:1-79:16 (2020). DOI: 10.1145/3386569.3392470<br>[4] F. Liu and E. Charbon, "1.8-µm pitch, 47-ps jitter SPAD array in a 130 nm SiGe BiCMOS process", Opt. Express 32(22), 38004-38012 (2024). DOI: 10.1364/OE.533631<br>[5] A.C. Ulku, C. Bruschini, I.M. Antolovic, Y. Kao, R. Ankri, S. Weiss, X. Michalet, E. Charbon, "A 512 × 512 SPAD Image Sensor With Integrated Gating for Widefield FLIM", IEEE Journal of Sel. Top. in Quantum El., 25(1), 6801212 (2019). DOI: 10.1109/JSTQE.2018.2867439<br>[6] K. Morimoto, A. Ardelean, M.-L. Wu, A.C. Ulku, I.M. Antolovic, C. Bruschini, E. Charbon, "A megapixel time-gated SPAD image sensor for 2D and 3D imaging applications", Optica 7(4), 346-354 (2020). DOI : 10.1364/OPTICA.386574</p>
				</div>
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Edoardo Charbon (SM'00 F'17) received the Diploma from ETH Zurich, the M.S. from the University of California at San Diego, and the Ph.D. from the University of California at Berkeley in 1988, 1991, and 1995, respectively, all in electrical engineering and EECS. He has consulted with numerous organizations, including Bosch, X-Fab, Texas Instruments, Maxim, Sony, Agilent, and the Carlyle Group. He was with Cadence Design Systems from 1995 to 2000, where he was the Architect of the company's initiative on information hiding for intellectual property protection. In 2000, he joined Canesta Inc., as the Chief Architect, where he led the development of wireless 3-D CMOS image sensors. Since 2002 he has been a member of the faculty of EPFL, where he is full professor. From 2008 to 2016 he was with Delft University of Technology's as full professor and Chair of VLSI design. He has been the driving force behind the creation of deep-submicron CMOS SPAD technology, which is mass-produced since 2015 and is present in telemeters, proximity sensors, and medical diagnostics tools. Since 2014, he has pioneered the use of Cryo-CMOS technology for the control of quantum devices, especially qubits, to achieve scalable, fault-tolerant quantum computing. His interests span from 3-D vision, LiDAR, FLIM, FCS, NIROT to super-resolution microscopy, time-resolved Raman spectroscopy, and cryo-CMOS circuits and systems for quantum computing. He has authored or co-authored over 500 papers and two books, and he holds 30 patents. Dr. Charbon is the recipient of the 2023 IISS Pioneering Achievement Award, he is a distinguished visiting scholar of the W. M. Keck Institute for Space at Caltech, a fellow of the Kavli Institute of Nanoscience Delft, a distinguished lecturer of the IEEE Photonics Society, and a fellow of the IEEE.</p>
				</div>
			</div>
		</div>
		<div class="box" id="austin" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="http://roorda.vision.berkeley.edu/">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/austin.png" alt="Image of Austin Roorda" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Austin Roorda</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">UC Berkeley</p>
						<h5 class="title is-5" style="color: #3273dc;">Oz Vision: A New Principle for Visual Display</h5>
					</div>
				</div>
			</div>
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>Humans have exquisite spatial and color vision despite what appear to be serious limits imposed by a seemingly suboptimal photoreceptor sensor array, an optical system that is fraught with aberrations, and an inability to hold the eye still, even during steady fixation. The Oz vision display can investigate the effects of these limits and overcome them. This is accomplished through a combination of adaptive optics, scanning light imaging and light delivery, and high-speed, accurate eye tracking. Collectively, these technologies enable control of the visual sensory input at the individual photoreceptor level at population scales. I will describe two experiments: For spatial vision, I will show how constant eye motion is actually leveraged by humans to improve fine spatial vision. For color vision, I will show how we can directly manipulate sensory input at the cone level to elicit color experiences - like 'olo' - that are outside the human gamut. I will finish with a broader discussion of ongoing and future experiments enabled by the Oz display.</p>
				</div>
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Austin Roorda received his Ph.D. in Vision Science/Physics from Waterloo in 1996. Since then, he has pioneered applications of adaptive optics for the eye, including mapping of the human trichromatic cone mosaic at the University of Rochester, inventing the adaptive optics scanning laser ophthalmoscope (AOSLO) at the University of Houston, and tracking and targeting light delivery to individual cones in the human eye at UC Berkeley. He's been at UC Berkeley since 2005 where he is a member of the Vision Science, Bioengineering and Neuroscience programs. He started a new position at the University of Waterloo in July 2025. He is a Fellow of Optica and ARVO. Notable awards include the Distinguished Alumni Award from Waterloo, the Glenn Fry Award from the American Academy of Optometry, a Guggenheim Fellowship, an Alcon Research Institute Award, a Leverhulme Visiting Professorship from Oxford University, the Rank Prize in Optoelectronics and the Boynton Prize from Optica.</p>
				</div>
			</div>
		</div>
		<div class="box" id="raquel" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://www.cs.toronto.edu/~urtasun/">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/raquel.png" alt="Image of Raquel Urtasun" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Raquel Urtasun</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">Waabi/University of Toronto</p>
						<h5 class="title is-5" style="color: #3273dc;">TBD</h5>
					</div>
				</div>
			</div>
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>TBD</p>
				</div>
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>TBD</p>
				</div>
			</div>
		</div>
	</section>

	<section class="subsection">
		<h3 class="subtitle">Invited Speakers</h3>

				<div class="box" id="laura" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://www.laurawaller.com/">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/laura.png" alt="Image of Laura Waller" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Laura Waller</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">UC Berkeley</p>
						<h5 class="title is-5" style="color: #3273dc;">Space-time methods for super-resolution microscopy of dynamic samples</h5>
					</div>
				</div>
			</div>
			
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>Computational imaging is permeating cameras and microscopes across many scientific applications, enabling new high-resolution and multi-dimensional measurement capabilities (e.g. phase, 3D, hyperspectral). But many methods require acquisition of multiple images to reconstruct this new information, limiting their applicability for live dynamic samples, where motion blur can cause severe artifacts. This talk will describe new space-time algorithms that correct for motion artifacts and solve for dynamics, with imperfect optical systems or approximate forward models. Traditional model-based image reconstruction algorithms work together with neural networks to optimize the inverse problem solver and the data capture strategy.</p>
				</div>
				
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Laura Waller is the Charles A. Desoer Professor of Electrical Engineering and Computer Sciences at UC Berkeley. She received B.S., M.Eng. and Ph.D. degrees from the Massachusetts Institute of Technology in 2004, 2005 and 2010. After that, she was a Postdoctoral Researcher and Lecturer of Physics at Princeton University from 2010-2012. She is a Packard Fellow for Science &amp; Engineering, Moore Foundation Data-driven Investigator, OSA Fellow, and Chan-Zuckerberg Biohub Investigator. She has received the Carol D. Soc Distinguished Graduate Mentoring Award, OSA Adolph Lomb Medal, the SPIE Early Career Award and the Max Planck-Humboldt Medal.</p>
				</div>
			</div>
		</div>
				<div class="box" id="bill" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://billf.mit.edu/">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/bill.png" alt="Image of Bill Freeman" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Bill Freeman</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">MIT</p>
						<h5 class="title is-5" style="color: #3273dc;">The Sensor Independence Assumption</h5>
					</div>
				</div>
			</div>
			
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>For imaging problems in science or medicine, you'd like to rely on prior assumptions about images as little as possible, when making image reconstructions from underdetermined sensor measurements. One often imposes the constraint that light is positive, which doesn't impose any disputable prior assumption about the images we're reconstructing. There's another indisputable assumption that I think is under-exploited: statistical independence between the sensor and the world. I'll show the power of this assumption for some underdetermined linear inverse problems.</p>
				</div>
				
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>William T. Freeman is the Thomas and Gerd Perkins Professor of Electrical Engineering and Computer Science (EECS) at MIT. He is interested in mid-level vision, audio, and computational photography, and co-authored a computer vision textbook recently published by MIT Press. He received outstanding paper awards at computer vision or machine learning conferences in 1997, 2006, 2009, 2012 and 2019, and test-of-time awards for papers from 1990, 1995, 2002, 2005, and 2012. He is a member of the National Academy of Engineering, and a Fellow of the IEEE, ACM, and AAAI. In 2019, he received the PAMI Distinguished Researcher Award, the highest award in computer vision.</p>
				</div>
			</div>
		</div>
				<div class="box" id="roarke" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://bme.duke.edu/people/roarke-horstmeyer/">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/roarke.png" alt="Image of Roarke Horstmeyer" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Roarke Horstmeyer</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">Duke University</p>
						<h5 class="title is-5" style="color: #3273dc;">High-throughput computational imaging with multi-camera array microscopes</h5>
					</div>
				</div>
			</div>
			
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>This talk details several new imaging approaches for extremely high-throughput capture of images and video at the microscopic scale. Due to their limited field-of-view, today's microscopes are relatively inefficient at imaging dynamics (e.g., living organisms, live cells) across large areas and volumes. This is particularly true for 3D recordings of freely moving small model organisms like the zebrafish and fruit fly, as well as in vitro cultures such as organoids and organospheres. This work presents a multi-camera array microscope (MCAM) that contains dozens of tightly packed imaging systems that can synchronously record video at multiple gigabytes per second data rates. In this talk, I will highlight several unique imaging configurations that are optimized to operate with the MCAM platform to unlock new scientific measurement regimes and large speed and efficiency gains for high-throughput imaging workflows.</p>
				</div>
				
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Roarke Horstmeyer is an assistant professor of Biomedical Engineering and Electrical and Computer Engineering at Duke University. He is also the Scientific Director at Ramona Optics. He develops microscopes, cameras and computer algorithms for a wide range of applications, from forming large-area, high-resolution 3D videos of freely moving organisms to detecting blood flow and brain activity deep within tissue. Dr. Horstmeyer's lab currently performs research within the fields of ptychography, high-content microscopic imaging, physics informed machine learning algorithms, and biophotonic measurement systems. Before joining Duke in 2018, Dr. Horstmeyer was a visiting professor at the University of Erlangen in Germany and an Einstein International Postdoctoral Fellow at Charité Medical School in Berlin. Prior to his time in Germany, Dr. Horstmeyer earned a PhD from Caltech's Electrical Engineering department (2016), an MS from the MIT Media Lab (2011), and bachelor's degrees in Physics and Japanese from Duke in 2006.</p>
				</div>
			</div>
		</div>
				<div class="box" id="ashok" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://profiles.rice.edu/faculty/ashok-veeraraghavan">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/ashok.png" alt="Image of Ashok Veeraraghavan" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Ashok Veeraraghavan</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">Rice University</p>
						<h5 class="title is-5" style="color: #3273dc;">Optics, sensors and AI: synergic computational imaging to go beyond the limits imposed by conventional imaging</h5>
					</div>
				</div>
			</div>
			
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>In this talk, I will discuss about several projects in my lab at the confluence of optics, sensors and artificial intelligence. In particular, I will provide examples of how co-designing sensors, optics and AI algorithms results in superior performance capabilities for imaging systems. I will provide a few example projects: (1) how co-designing imaging optics along with AI algorithms can enable high-throughput 3D imaging, and microscopy, (2) how novel diffractive and meta-optical elements allow us to realize imaging systems with novel functionalities and form-factors and finally time-permitting, (3) how emerging neural representations along with high resolution spatial light modulators can allow us to image through thick scattering media without the need for guidestars. I will use these projects to argue that we should look at the three computational blocks within an imaging system, optics, sensors and algorithms together and that co-designing them can result in significant performance improvements over the state of art.</p>
				</div>
				
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Ashok Veeraraghavan serves as the current Chair of the Department of Electrical and Computer Engineering. He also directs the computational imaging lab, which focuses on solving hard and challenging problems in imaging and vision by co-designing sensors, optics, electronics, signal processing, and machine learning algorithms. Before joining Rice University, he spent three wonderful and fun-filled years as a Research Scientist at Mitsubishi Electric Research Labs in Cambridge, MA. He received his Bachelors in Electrical Engineering from the Indian Institute of Technology, Madras in 2002 and M.S and PhD. degrees from the Department of Electrical and Computer Engineering at the University of Maryland, College Park in 2004 and 2008 respectively. His thesis received the Doctoral Dissertation award from the Department of Electrical and Computer Engineering at the University of Maryland. His work has won numerous awards including the Peter and Edith O'Donnel Award for Engineering in 2024, Charles Duncan Innovation Award 2019, Hershel. M. Rich Invention Award in 2016 and 2017, and an NSF CAREER award in 2017. He loves playing, talking, and pretty much anything to do with the slow and boring but enthralling game of cricket.</p>
				</div>
			</div>
		</div>
				<div class="box" id="shwetadwip" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://www.ece.utexas.edu/people/faculty/shwetadwip-chowdhury">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/shwetadwip.png" alt="Image of Shwetadwip Chowdhury" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Shwetadwip Chowdhury</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">UT Austin</p>
						<h5 class="title is-5" style="color: #3273dc;">Exploring the nonconvex landscape of biological inverse-scattering</h5>
					</div>
				</div>
			</div>
			
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>Optical imaging is a major research tool in the basic sciences, but is limited by tissue scattering to short imaging depths. This prevents large-scale bio-imaging by allowing visualization of only the outer superficial layers of an organism, or specific components isolated from within the organism and prepared in-vitro. Our lab develops inverse-scattering techniques to computationally correct for the effects of tissue scattering and reconstruct high-resolution volumetric images of the sample. However, these methods are nonconvex and susceptible to local minima. We demonstrate that reconstruction quality is critically, and sometimes counterintuitively, influenced by factors such as the choice between field- and amplitude-based cost functions, computational padding, initialization strategies using weak-scattering assumptions, and the application of refocusing. Lastly, we present the methodology that we found through empirical testing has proven most effective for robust and high-quality 3D imaging of diverse biological scattering samples.</p>
				</div>
				
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Shwetadwip Chowdhury is an assistant professor in the Department of Electrical and Computer Engineering at the University of Texas at Austin. His research interests are in developing next generation optical imaging technologies for applications in science and medicine.</p>
				</div>
			</div>
		</div>
				<div class="box" id="rafael" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://www.colorado.edu/ecee/rafael-piestun">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/rafael.png" alt="Image of Rafael Piestun" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Rafael Piestun</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">University of Colorado Boulder</p>
						<h5 class="title is-5" style="color: #3273dc;">Computational Imaging enables ultra-thin endomicroscopy</h5>
					</div>
				</div>
			</div>
			
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>In-vivo imaging using hair-thin multimode fibers has been recently accomplished. Multimode fibers are attractive for endoscopic applications due to their thin cross-section, high modal density, and flexibility. However modal dispersion and intermodal coupling preclude direct image transmission. Computational imaging techniques are thus critical to enable full functionality. The development of fast spatial phase control enables focus scanning and structured illumination for different imaging modalities. We discuss the implications of these techniques for ultrathin optical endoscopy and show the latest in-vivo implementations.</p>
				</div>
				
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Prof. Rafael Piestun received the Ingeniero Electricista degree from the Universidad de la República (Uruguay) and MSc. and Ph.D. degrees in Electrical Engineering from the Technion – Israel. From 1998 to 2000 he was a researcher at Stanford University. Since 2001 he has been at the University of Colorado Boulder where he is a professor in the department of Electrical and Computer Engineering and in the Physics department. He is a fellow of the Optical Society of America, was a Fulbright scholar, an Eshkol fellow, received a Honda Initiation Grant award, a Minerva award, a Provost Achievement Award, and El-Op and Gutwirth prizes. He was associate editor of Optics and Photonics News and Applied Optics. He is founder of the company Double Helix Optics (SPIE Prism Award, First Place in the Luminate Competition) and the company Modendo Inc. His areas of interest include computational optical imaging, superresolution microscopy, volumetric photonic devices, scattering optics, and ultrafast optics.</p>
				</div>
			</div>
		</div>
				<div class="box" id="vivek" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://www.bu.edu/eng/profile/vivek-goyal/">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/vivek.png" alt="Image of Vivek Goyal" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Vivek Goyal</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">Boston University</p>
						<h5 class="title is-5" style="color: #3273dc;">Quantitative Secondary Electron Yield Mapping in Ion-Beam Microscopy</h5>
					</div>
				</div>
			</div>
			
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>Secondary electron imaging (SEI) modalities, such as scanning electron microscopy (SEM) and helium ion microscopy (HIM), are widely used for nanoscale visualization. They are known to suffer from super-Poissonian noise and are generally qualitative because of unknown detector parameters. My group has been working to develop SEI as a quantitative modality with robustness to various sources of noise. The central idea is to recognize that there are phenomena within each pixel dwell time that make it worthwhile to measure more than a single scalar value during that dwell time. Recently, we have shown how to use our central idea with an existing HIM instrument, without replacing its noisy Everhart-Thornley detector. With ion count-aided microscopy [10.1073/pnas.2401246121], we demonstrate an improvement in the dose-accuracy tradeoff that enables dose reduction by a factor between 2 and 3. Theory predicts much larger improvements are possible with heavier incident ions or in low-voltage SEM. Our improvement represents a near elimination of source shot noise, and ongoing work addresses beam spot size-related limitations to resolution.</p>
				</div>
				
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Vivek Goyal received undergraduate degrees in electrical engineering and mathematics from the University of Iowa. After graduate degrees in electrical engineering from UC-Berkeley, he was a Member of Technical Staff at Bell Laboratories, a Senior Research Engineer for Digital Fountain, and the Esther and Harold E. Edgerton Associate Professor of Electrical Engineering at MIT. His research group spawned 3dim Tech, winner of the 2013 MIT $100K Entrepreneurship Competition Launch Contest Grand Prize, and he was consequently with Google/Alphabet Nest Labs 2014-2016. He is now a Professor of Electrical and Computer Engineering at Boston University. Dr. Goyal is a Fellow of the AAAS, IEEE, and Optica, and he and his students have been awarded ten IEEE paper awards and eight thesis awards. He is a co-author of Foundations of Signal Processing (Cambridge Univ. Press, 2014) and a 2024 Guggenheim Fellow.</p>
				</div>
			</div>
		</div>
				<div class="box" id="matthew" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://www.cs.cmu.edu/~motoole2/">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/matt.png" alt="Image of Matthew O'Toole" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Matthew O'Toole</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">Carnegie Mellon University</p>
						<h5 class="title is-5" style="color: #3273dc;">Spatially-selective Lensing</h5>
					</div>
				</div>
			</div>
			
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>A camera lens brings a single plane into focus on a sensor; parts of the scene not on this plane are resolved on the sensor under defocus. Can we break this precept by enabling a lens that changes its depth-of-field arbitrarily? This presentation discusses the design and implementation of a lens capable of spatially-selective focusing. Through a combination of cubic phase plates and spatial light modulators, we demonstrate control over focus at the pixel level. This confers unique imaging capabilities---most notably, the ability to bring an entire scene into sharp focus without reducing the size of the lens's aperture.</p>
				</div>
				
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Matthew O'Toole is an Associate Professor with the Robotics Institute and Computer Science Department at Carnegie Mellon University. Prior to joining CMU, he was a Banting Postdoctoral Scholar at Stanford University, and received his Ph.D. from the University of Toronto. He is also the recipient of an NSF CAREER award; a ACM SIGGRAPH Outstanding Thesis Honourable Mention award; best paper awards at SIGGRAPH 2023, CVPR 2022, CVPR 2014, and ICCV 2007; and best demo awards at ICCP 2023, CVPR 2015, and ICCP 2015.</p>
				</div>
			</div>
		</div>
				<div class="box" id="liang" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://samueli.ucla.edu/people/liang-gao/">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/liang.png" alt="Image of Liang Gao" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Liang Gao</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">UCLA</p>
						<h5 class="title is-5" style="color: #3273dc;">Breaking the Speed Barrier: High-Speed Light-Field Microscopy for Kilohertz 3D Imaging and beyond</h5>
					</div>
				</div>
			</div>
			
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>Light-field microscopy offers unprecedented capabilities for 3D visualization of biological samples down to the sub-cellular level. Despite its strengths, the technique has historically faced limitations in imaging speed due to the acquisition of large-format light field data. This bottleneck has restricted its use in high-speed bioimaging applications like voltage imaging or fluorescence lifetime imaging microscopy (FLIM). In this presentation, I will discuss our innovative approaches to overcoming this challenge by leveraging advancements in computational optics and detector technology, enabling a volumetric frame rate from kilohertz to terahertz.</p>
				</div>
				
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Dr. Liang Gao is an Associate Professor of Bioengineering at UCLA, specializing in optical imaging and its application in biomedicine. He has authored over 80 peer-reviewed publications in top-tier journals, including Nature, Nature Communications, Science Advances, and PNAS. Dr. Gao received his BS degree in Physics from Tsinghua University in 2005 and his PhD in Applied Physics and Bioengineering from Rice University in 2011. He has been honored with several awards, including a CAREER award from NSF, a MIRA award from NIH, a Frontiers of Science Award in Computational Optics from ICBS, and a Harold E. Edgerton Award in High-Speed Optics from SPIE. Dr. Gao is also a fellow member of SPIE and Optica (formerly OSA).</p>
				</div>
			</div>
		</div>
				<div class="box" id="edward" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://bcs.mit.edu/directory/edward-adelson">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/ted.png" alt="Image of Edward Adelson" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Edward Adelson</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">Massachusetts Institute of Technology</p>
						<h5 class="title is-5" style="color: #3273dc;">Capturing touch and surface geometry with vision based tactile sensors</h5>
					</div>
				</div>
			</div>
			
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>A vision based tactile sensor (VBTS) consists of a soft gel pad covered with a reflective membrane, viewed by an internal camera. My lab builds GelSight sensors, which capture the distortion of the membrane though photometric stereo using internal LED's. The native language of a VBTS is geometry: when the sensor presses on a surface we can track the X,Y,Z distortion of every point on the membrane over time. With a GelSight robot fingertip we can measure force, but usually geometry is more important than force. The soft robot fingertip lets us detect slip, identify objects and materials, determine object pose, and estimate material properties like hardness. GelSight-based instruments can also be used to measure surface microgeometry with optically challenging materials (like glass or metal) and challenging circumstances (like on the wing of an airplane).</p>
				</div>
				
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Edward Adelson is the John and Dorothy Wilson Professor of Vision Science at MIT, in the Department of Brain and Cognitive Sciences, and a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL). He is a member of the National Academy of Sciences. He has published widely on topics in human vision, computer vision, computer graphics, artificial tactile sensing, and robotics. He is well known for contributions to multiscale image representation (such as the Laplacian pyramid) and basic concepts in early vision such as steerable filters and motion energy models. He introduced the plenoptic function, and built the first plenoptic camera. He has produced some well known illusions such as the Checker-Shadow Illusion. He has been honored with the Nakayama Medal of the Vision Sciences Society, two test-of-time awards from the IEEE Computer Society, and the IEEE Lifetime Achievement Award in Computer Vision. He now works on artificial touch sensing for robotics.</p>
				</div>
			</div>
		</div>
				<div class="box" id="shree" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://www.cs.columbia.edu/~nayar/">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/shree.png" alt="Image of Shree Nayar" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Shree Nayar</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">Columbia University</p>
						<h5 class="title is-5" style="color: #3273dc;">Vision for Solar Energy</h5>
					</div>
				</div>
			</div>
			
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>In urban areas, solar panels are mounted on rooftops, on the sides of buildings, at the ground level, on poles, and even embedded within windows. The market for urban solar panels was 64 billion is 2025 and is projected to double by 2035. What makes the use of a solar panel in an urban setting interesting is that, due to occlusions, the panel sees only a fraction of the sky and hence receives direct sunlight for only a part of the day, and maybe even never. An important question therefore is: For any given urban location, how should a panel be oriented to maximize its irradiance and hence the energy it harvests? We present a minimal sensing method for automatically orienting a solar panel in the direction of maximum irradiance, irrespective of the complexity of the illumination it receives. We also present a vision-based approach for forecasting the energy harvested by an already installed panel, which is valuable information for managing supply and demand in an electric grid.</p>
				</div>
				
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Shree K. Nayar is the T. C. Chang Professor of Computer Science at Columbia University. He heads the Columbia Imaging and Vision Laboratory (CAVE), which develops computational imaging and computer vision systems. Nayar received his PhD degree in Electrical and Computer Engineering from the Robotics Institute at Carnegie Mellon University. For his research and teaching he has received several honors including the David Marr Prize (1990 and 1995), the David and Lucile Packard Fellowship (1992), the National Young Investigator Award (1993), the NTT Distinguished Scientific Achievement Award (1994), the Keck Foundation Award for Excellence in Teaching (1995), the Columbia Great Teacher Award (2006), the Carnegie Mellon Alumni Achievement Award (2009), Sony Appreciation Honor (2014), the Columbia Engineering Distinguished Faculty Teaching Award (2015), the IEEE PAMI Distinguished Researcher Award (2019), the Funai Achievement Award (2021), and the Okawa Prize (2022). For his contributions to computer vision and computational imaging, he was elected to the National Academy of Engineering in 2008, the American Academy of Arts and Sciences in 2011, and the National Academy of Inventors in 2014.</p>
				</div>
			</div>
		</div>
				<div class="box" id="mohit" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://wisionlab.com/people/mohit-gupta/">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/mohit.png" alt="Image of Mohit Gupta" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Mohit Gupta</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">University of Wisconsin-Madison</p>
						<h5 class="title is-5" style="color: #3273dc;">Quanta Computational Imaging</h5>
					</div>
				</div>
			</div>
			
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>Single-photon avalanche diodes (SPADs) are an emerging sensor technology that promises single-photon sensitivity and the ability to time-tag photons with picosecond precision. In the last few years, we have witnessed a single-photon revolution, resulting for the first time in multi-megapixel SPAD arrays, which allow capturing sufficient spatial details needed for mainstream computer vision applications, including object detection and tracking, text recognition, pose recognition, and SLAM. This raises the following questions: Are SPADs ready to be used beyond niche scientific applications, as general-purpose computer vision sensors? Can they be deployed outside the lab in-the-wild? If so, what are their benefits over conventional cameras? We will discuss the opportunities for (and open challenges in) developing ``quanta'' imaging / perception algorithms for extracting scene information from SPAD photon streams, with the goal of performing robust vision in extreme conditions including rapid motion, low light (~0.1 lux) and high dynamic range.</p>
				</div>
				
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Mohit Gupta is an Associate Professor of Computer Sciences at the University of Wisconsin-Madison. He received a Ph.D. from the Robotics Institute, Carnegie Mellon University, and was a postdoctoral research scientist at Columbia University. He directs the WISION Lab with research interests in computer vision and computational imaging. He has received best paper honorable mention awards at computer vision and photography conferences, including a Marr Prize honorable mention at IEEE ICCV, a best demo award at SIGGRAPH E.Tech. 2024, Sony Faculty Innovation Awards and an NSF CAREER award. His research is supported by NSF, ONR, DARPA, Sony, Snap, and Intel.</p>
				</div>
			</div>
		</div>
				<div class="box" id="srinivasa" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://www.cs.princeton.edu/~fheide/">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/srinivasa.png" alt="Image of Srinivasa Narasimhan" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Srinivasa Narasimhan</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">Carnegie Mellon University</p>
						<h5 class="title is-5" style="color: #3273dc;">Driving in Bad Weather and through Workzones</h5>
					</div>
				</div>
			</div>
			
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>Driver-assist and autonomous driving systems have come a long way since the DARPA grand challenges but we still keep hearing of accidents in difficult conditions including bad weather, night and work zones. Thousands of fatalities happen in such conditions every year even in regions and countries where transportation infrastructure is advanced. In this talk, I will present some of our group's works on how to generate and understand road scenes in such conditions by leveraging and controlling large generative models and augmenting with diverse data we have been collecting.</p>
				</div>
				
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Srinivasa Narasimhan is the U.A. and Helen Whitaker Professor of Robotics at Carnegie Mellon University. He currently serves as Interim Director of the Robotics Institute leading a department of over a thousand students, staff and faculty. He obtained his PhD from Columbia University in Dec 2003. His group focuses on novel techniques for imaging and illumination to enable applications in vision, graphics, robotics, agriculture, intelligent transportation and medical imaging. His works have received over a dozen Best Paper or Best Demo or Honorable mention awards at major conferences [IV (2021), ICCV (2013), CVPR (2022, 2019, 2015, 2000), ICCP (2020, 2015, 2012), I3D (2013), CVPR/ICCV Workshops (2007, 2009)]. In addition, he has received the Ford URP Award (2013), Okawa Research Grant (2009) and the NSF CAREER Award (2007). He is the co-inventor of programmable headlights, Aqualux 3D display, Assorted-pixels, Motion-aware cameras, Episcan360, Episcan3D, EpiToF3D, and programmable triangulation light curtains. Hi served on the editorial board of the International Journal of Computer Vision (2009-2023) and serves frequently as Senior or Lead Area Chair of top computer vision conferences (CVPR, ICCV, ECCV, BMVC, ACCV, 3DV).</p>
				</div>
			</div>
		</div>
				<div class="box" id="gordon" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://stanford.edu/~gordonwz/">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/gordon.png" alt="Image of Gordon Wetzstein" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Gordon Wetzstein</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">Stanford University</p>
						<h5 class="title is-5" style="color: #3273dc;">Holographic Near-eye Displays</h5>
					</div>
				</div>
			</div>
			
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>Holographic near-eye displays are an emerging platform for virtual and augmented reality systems. Their unique ability to control the phase of a displayed wavefront enables aberrations of downstream optics, such as waveguide combiners, to be corrected by the light engine. This ability opens up a new design space for ultra-compact AR/VR display systems. In this talk, we review recent advances in compact holographic display systems with large etendue, efficient AI-based calibration techniques of these systems, and modern computer-generated holography algorithms that leverage Gaussian splatting for wave optics-based holographic displays.</p>
				</div>
				
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Gordon Wetzstein is an Associate Professor of Electrical Engineering and, by courtesy, of Computer Science at Stanford University. He is the director of the Stanford Computational Imaging Lab and a faculty director of the Stanford Center for Image Systems Engineering. At the intersection of computer graphics and vision, artificial intelligence, computational optics, and applied vision science, Prof. Wetzstein's research has a wide range of applications in next-generation imaging, wearable computing, and neural rendering systems. Prof. Wetzstein is a Fellow of Optica and the IEEE, and the recipient of several awards, including an IEEE VGTC Virtual Reality Technical Achievement Award, an NSF CAREER Award, an Alfred P. Sloan Fellowship, an ACM SIGGRAPH Significant New Researcher Award, a Presidential Early Career Award for Scientists and Engineers (PECASE), an SPIE Early Career Achievement Award, an Electronic Imaging Scientist of the Year Award, an Alain Fournier Ph.D. Dissertation Award as well as several Best Paper and Demo Awards.</p>
				</div>
			</div>
		</div>
				<div class="box" id="wolfgang" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://vccimaging.org/People/heidriw/">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/wolfgang.png" alt="Image of Wolfgang Heidrich" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Wolfgang Heidrich</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">KAUST</p>
						<h5 class="title is-5" style="color: #3273dc;">Learned and Domain Specific Imaging Systems</h5>
					</div>
				</div>
			</div>
			
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>Computational imaging systems are based on the joint design of optics and associated image reconstruction algorithms. Of particular interest in recent years has been the development of end-to-end learned "Deep Optics" systems that use differentiable optical simulation in combination with backpropagation to simultaneously learn optical design and deep network post-processing for applications such as hyperspectral imaging, HDR, or extended depth of field. In this talk I will in particular focus on new developments that expand the design space of such systems from simple DOE optics to compound refractive optics and mixtures of different types of optical components. I will also highlight how to design application-specific imaging systems that require very low hardware resources using the power of generative models.</p>
				</div>
				
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Wolfgang Heidrich is a Professor of Computer Science and Electrical and Computer Engineering in the KAUST Visual Computing Center, for which he also served as director from 2014 to 2021. Prof. Heidrich joined King Abdullah University of Science and Technology (KAUST) in 2014, after 13 years as a faculty member at the University of British Columbia. He received his PhD in from the University of Erlangen in 1999, and then worked as a Research Associate in the Computer Graphics Group of the Max-Planck-Institute for Computer Science in Saarbrucken, Germany, before joining UBC in 2000. Prof. Heidrich's research interests lie at the intersection of imaging, optics, computer vision, computer graphics, and inverse problems. His more recent interest is in computational imaging, focusing on hardware-software co-design of the next generation of imaging systems, with applications such as High-Dynamic Range imaging, compact computational cameras, hyperspectral cameras, to name just a few. Prof. Heidrich is a Fellow of the National Academy of Inventors, IEEE, Optica, AAIA, and Eurographics, and the recipient of a Humboldt Research Award as well as the ACM SIGGRAPH Computer Graphics Achievement Award.</p>
				</div>
			</div>
		</div>
				<div class="box" id="ioannis" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://users.ece.cmu.edu/~igkioule/">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/ioannis.png" alt="Image of Ioannis Gkioulekas" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Ioannis Gkioulekas</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">Carnegie Mellon University</p>
						<h5 class="title is-5" style="color: #3273dc;">A ray tracer for physics</h5>
					</div>
				</div>
			</div>
			
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>Ray tracing has revolutionized science and engineering, by enabling the simulation of light at unprecedented scale and generality. Yet how we simulate light is in stark contrast with how we simulate other physical phenomena, modeled through general partial differential equations, for which methods such as finite elements have been the gold standard for decades. The reliance of these methods on grid-based discretization fundamentally limits how much we can scale or generalize physics simulation, in turn hindering progress across all areas of science and engineering. I will discuss ongoing work to change this state of affairs, by creating a new methods for simulation of partial differential equations that achieve the same scale and generality as ray tracing. As with ray tracing, the key insight behind these algorithms is to elide discretization through Monte Carlo estimation. Our Monte Carlo simulation methods are embarrassingly parallelizable, scale to massive scene specifications, enable differentiability and volumetrization, and create exciting new opportunities for utilizing simulation in computational imaging.</p>
				</div>
				
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Ioannis Gkioulekas is an associate professor in the Robotics Institute at Carnegie Mellon University. He works broadly in computer graphics and computer vision, focusing on computational imaging: the joint design of optics, electronics, and computation, to achieve unprecedented imaging capabilities. Technical keywords that often show up in his research include: interferometry, acousto-optics, single-photon imaging, lidar, speckle, physics-based rendering, differentiable rendering, volume rendering, Monte Carlo simulation. For his research he has received the NSF CAREER Award, Sloan Research Fellowship, Bodossaki Distinguished Young Scientist Award, and three best paper awards at CVPR and SIGGRAPH.</p>
				</div>
			</div>
		</div>
				<div class="box" id="aswin" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://users.ece.cmu.edu/~saswin/">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/aswin.png" alt="Image of Aswin Sankaranarayanan" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Aswin Sankaranarayanan</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">Carnegie Mellon University</p>
						<h5 class="title is-5" style="color: #3273dc;">PlatonicVR: Exploiting Human Visual Properties for Ultra-Wide Field of View Displays</h5>
					</div>
				</div>
			</div>
			
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>Matching the formidable capabilities of the human visual system remains one of the enduring challenges in display research. A fundamental limitation in current stereoscopic displays is the coupling between eyebox extent and field of view, which arises from the eye's use of rotational motion to explore its visual field. This coupling prevents conventional approaches from achieving the ~200° horizontal field of view of human vision without prohibitive complexity. In this talk, I will present PlatonicVR, a novel stereoscopic display architecture that exploits specific properties of human visual perception to dramatically simplify ultra-wide field of view display design. By leveraging these perceptual characteristics, our approach decouples the eyebox-field of view relationship and enables practical implementation of displays that approach the human eye's natural viewing capabilities. I will discuss the underlying optical principles, demonstrate the effectiveness of our approach, and explore implications for next-generation VR systems.</p>
				</div>
				
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Aswin Sankaranarayanan is a professor in the ECE department at CMU, where he leads the Image Science Lab. His research interests are broadly in computational photography, signal processing and computer vision. His doctoral research was in the University of Maryland where his dissertation won the distinguished dissertation award from the ECE department in 2009. Aswin is also the recipient of the NSF CAREER award, and best paper awards at SIGGRAPH, CVPR and ICCP.</p>
				</div>
			</div>
		</div>
				<div class="box" id="felix" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://www.cs.princeton.edu/~fheide/">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/felix.png" alt="Image of Felix Heide" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Felix Heide</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">Princeton University</p>
						<h5 class="title is-5" style="color: #3273dc;">Cameras As Nanophotonic Optical Computers</h5>
					</div>
				</div>
			</div>
			
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>Presentation Abstract: Cameras have become a ubiquitous interface between the real world and computers, with applications across domains in fundamental science, robotics, health, and communication. Although their applications are diverse, today's cameras acquire information in the same way they did in the 19th century: they focus light from the scene on a sensing plane using a set of refractive lenses that minimize deviations from Gauss's linear model of optics. In this paradigm, increasingly complex lens and sensor stacks are designed to record an ideal image and perform computation only after the capture. For example, the optical stack in the iPhone 16 contains more than seven elements at 7 mm in length. In this talk, I will discuss computational cameras that learn to manipulate the wavefront of incident light with wavelength-scale structuring, previously impractical to design with existing electromagnetic wave simulation methods. These neural nanophotonic cameras may enable unprecedented capabilities in optical design, imaging, and computer vision. As examples, I will describe an ultra-small camera at a few hundred microns in size that matches the quality achieved with cm-size compound lenses. I will also present ultra-thin nanophotonic cameras that perform 99.9% of neural network compute - typically executed in electronics after the capture - in the optics before sensing, at the speed of light.</p>
				</div>
				
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Felix Heide is an Assistant Professor at Princeton University, Head of AI at Torc Robotics, and Founder self-driving vehicle startup Algolux (now part of Diamler Trucks and Torc Robotics). He is researching the theory and application of computational imaging and computer vision systems. Exploring imaging, vision, and display systems end-to-end, Felix's work lies at the intersection of optics, machine learning, optimization, computer graphics, and computer vision. He received his Ph.D. from the University of British Columbia. Felix obtained his undergraduate degree from the University of Siegen and was a postdoc at Stanford University. His doctoral dissertation won the Alain Fournier Dissertation Award and the SIGGRAPH outstanding doctoral dissertation award. He won the NSF CAREER Award 2021 and the Sony Young Faculty Award 2021. He was named a Packard Fellow in 2022 and a Sloan Research Fellow in 2023. Felix was named SIGGRAPH New Significant Researcher in 2023. He founded the autonomous driving startup Algolux, building on academic research during his Ph.D. thesis. With over 150 employees and venture capital of more than 30 Million USD, Algolux has shipped computer vision software to more than 1 Million vehicles of European automotive manufacturers.</p>
				</div>
			</div>
		</div>
				<div class="box" id="sabine" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://people.epfl.ch/sabine.susstrunk">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/sabine.png" alt="Image of Sabine Süsstrunk" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Sabine Süsstrunk</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">EPFL</p>
						<h5 class="title is-5" style="color: #3273dc;">Neural Cellular Automata: A New Paradigm for Self-Organizing Textures</h5>
					</div>
				</div>
			</div>
			
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>In nature, complex patterns such as animal coats, wood grain, and swirling clouds emerge when many simple elements locally interact. Cellular automata (CAs) capture this principle, and neural cellular automata (NCAs) advance it further by learning the local update rule directly from data while preserving locality and parallelism. With carefully designed architectures, we build NCAs that generate rich, evolving 2D and 3D textures in real time. The models are lightweight and run efficiently even on edge devices, thus enabling real-time interactive user control. Our approach not only simplifies the creation and control of intricate visual effects but also prove how NCAs offer a powerful, fundamentally new tool set for computer graphics and texture synthesis.</p>
				</div>
				
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Sabine Süsstrunk is Full Professor and Director of the Image and Visual Representation Lab in the School of Computer and Communication Sciences (IC) at the Ecole Polytechnique Fédérale (EPFL), Lausanne, Switzerland. Her main research areas are in computational photography and imaging, color computer vision, and computational image quality and aesthetics. Sabine is a Fellow of ELLIS, IEEE and IS&amp;T, and President of the Swiss Science Council (SSC). She organized ICCP 2024 in Lausanne, Switzerland.</p>
				</div>
			</div>
		</div>
				<div class="box" id="chrysanthe" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://umwa.memphis.edu/fcv/viewprofile.php?uuid=cpreza">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/chrysanthe.png" alt="Image of Chrysanthe Preza" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Chrysanthe Preza</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">The University of Memphis</p>
						<h5 class="title is-5" style="color: #3273dc;">Computational imaging enables efficient structured illumination microscopy</h5>
					</div>
				</div>
			</div>
			
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>Improving the performance of three-dimensional (3D) fluorescence microscopes is a topic that has received a lot of attention over the years. In this talk, I will discuss techniques that we have been developing to improve 3D-spatial resolution based on "optical-transfer function engineering" using novel structured illumination approaches and computational methods, including efficient image restoration using plug and play methods to integrate physical and learned models.</p>
				</div>
				
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Dr. Chrysanthe Preza is the Kanuri Professor and Chair of the Department of Electrical and Computer Engineering at the University of Memphis, where she joined in 2006. She received her D.Sc. degree in Electrical Engineering from Washington University in St. Louis in 1998. She leads the research in the Computational Imaging Research Laboratory. Since 2022, she is the founder and Director of the Vertically Integrated Projects (VIP) Program at The University of Memphis. Her research interests are imaging science, estimation theory, computational optical sensing and imaging applied to multidimensional multimodal light microscopy and hyperspectral imaging, and computational imaging enabled by deep learning. She received a CAREER award by the National Science Foundation in 2009, the Herff Outstanding Faculty Research Award in 2010 and 2015, and she was the recipient of the Ralph Faudree Professorship at the University of Memphis 2015-2018. Since 2018, she has been the recipient of the Ravi and Eleanor Kanuri Professorship. She was named Fellow of the SPIE in 2019 and Fellow of the Optica (OSA) in 2020. She served as Associate Editor for IEEE Transactions on Computational Imaging, as Topical Editor for Optica's Applied Optics, and as an Executive Editor for Biological Imaging, Cambridge University Press. She currently serves as an Editorial Board Member for Journal of Imaging, Computational Imaging and Computational Photography section.</p>
				</div>
			</div>
		</div>
				<div class="box" id="jinwei" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://cs.gmu.edu/~jinweiye/">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/jinwei.png" alt="Image of Jinwei Ye" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Jinwei Ye</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">George Mason University</p>
						<h5 class="title is-5" style="color: #3273dc;">Revisit Mirrors for 3D Imaging</h5>
					</div>
				</div>
			</div>
			
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>Mirrors are ubiquitous in imaging systems. In this talk, I'll revisit various mirror configurations for 3D imaging. I'll showcase a catadioptric lens design for snapshot 3D Gaussian Splatting and demonstrate 3D viewing results on hard-to-reconstruct tiny objects in motion, such as a crawling lady bug.</p>
				</div>
				
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Jinwei Ye is an associate professor of Computer Science at George Mason University. Before that, she was an assistant professor at Louisiana State University (2017–2021). She received her Ph.D. in Computer Science from the University of Delaware in 2014. Her research interests are at the intersection of computer vision, computational imaging, and computer graphics. Her works are supported by NSF and ARL. She received the NSF CAREER awards in 2023. She served in the senior program committee and organizing committee for major computer vision conferences, including CVPR, ICCV, ECCV, WACV, and ICCP.</p>
				</div>
			</div>
		</div>
				<div class="box" id="or" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://orpatashnik.github.io/">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/or.png" alt="Image of Or Patashnik" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Or Patashnik</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">Tel Aviv University</p>
						<h5 class="title is-5" style="color: #3273dc;">On Attention Layers for Image Generation and Manipulation</h5>
					</div>
				</div>
			</div>
			
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>Attention layers play a critical role in generative models. In this talk, I will show that these layers capture rich semantic information, and particularly semantic correspondences between elements within the image and across different images. Through several works, I will show that the rich representations learned by these layers can be leveraged for image manipulation, consistent image generation, and personalization. Additionally, I will discuss the challenges that arise, especially in scenarios involving complex prompts with multiple subjects. Specific issues, such as semantic leakage during the denoising process, can lead to inaccurate representations, resulting in poor generations. I will present methods that mitigate these issues, allowing the generation of complex images.</p>
				</div>
				
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Or Patashnik is a Senior Lecturer (Assistant Professor) at the School of Computer Science at Tel Aviv University. Her research lies at the intersection of computer graphics, computer vision, and machine learning, with a particular focus on generative models. She develops methods for image and video generation, semantic editing, and personalization, aiming to make visual content creation more controllable and expressive. Or completed her PhD at Tel Aviv University under the supervision of Prof. Daniel Cohen-Or. During her graduate studies, she also spent time at Carnegie Mellon University as a visiting scholar, collaborating with Prof. Fernando De la Torre and Prof. Jun-Yan Zhu. Her work has been recognized with several awards, including the Blavatnik Prize for Outstanding PhD Students, the Shashua Family Foundation Scholarship, and the Celia and Marcos Maus Annual Prize.</p>
				</div>
			</div>
		</div>
				<div class="box" id="sara" style="margin-bottom: 2rem;">
			<!-- Profile and Title Row -->
			<div class="columns is-variable is-0" style="margin-bottom: 1.5rem;">
				<div class="column is-3">
					<a href="https://sarafridov.github.io/">
						<figure class="image">
							<img class="is-rounded" src="./static/img/speakers/sara.png" alt="Image of Sara Fridovich-Keil" style="width: 150px; height: 150px; object-fit: cover;">
						</figure>
					</a>
				</div>
				<div class="column is-9 is-flex is-align-items-center">
					<div class="content">
						<h4 class="title is-4">Sara Fridovich-Keil</h4>
						<p class="subtitle is-6" style="margin-bottom: 2rem;">Georgia Tech</p>
						<h5 class="title is-5" style="color: #3273dc;">Volume Representations for Inverse Problems</h5>
					</div>
				</div>
			</div>
			
			<!-- Full-width Abstract and Bio -->
			<div class="content">
				<div class="abstract-section" style="margin-bottom: 1.5rem;">
					<h6 class="title is-6">Abstract</h6>
					<p>We introduce a family of models, Geometric Algebra Planes, that is the first class of Implicit Neural Representation (INR) trainable by convex optimization. GA-Planes models generalize many existing representations, and have a natural interpretation as a low-rank plus low-resolution matrix factorization. In 3D, we demonstrate GA-Planes' competitive performance in terms of expressiveness, model size, and optimizability across three volume fitting tasks including radiance field reconstruction. I will conclude with a benchmarking comparison of many INRs against a simple interpolated grid baseline, that highlights remaining limitations of INRs and open problems in 3D signal representation for inverse problems.</p>
				</div>
				
				<div class="bio-section">
					<h6 class="title is-6">Bio</h6>
					<p>Sara Fridovich-Keil is an Assistant Professor at Georgia Tech in the School of Electrical and Computer Engineering, and program faculty in Machine Learning. Before joining Georgia Tech, she was a postdoc at Stanford and completed her PhD at UC Berkeley. Her research focuses on foundations and applications of machine learning and signal processing in computational imaging.</p>
				</div>
			</div>
		</div>
	</section>
</section>